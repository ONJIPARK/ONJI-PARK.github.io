---
layout: single
title:  "6. Team - Cosine Similarities"
categories: coding
tag: [python, blog, jupyter, Cosine Similarity, Word2Vec]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>



```python
# pip install gensim
# !pip install gensim
# !pip install -upgrade gensim
```


```python
import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
import re
import numpy as np
import nltk
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
```


```python
import gensim.downloader as api
word2vec_model = api.load('word2vec-google-news-300')
```

---



```python
def junchuri(text) : 
    "".join(i for i in text if  ord(i)<128)
    
    text.lower()
    
    text = text.split()
    stops = set(stopwords.words("english"))
    text = [w for w in text if not w in stops]
    text = " ".join(text)

    html_pattern = re.compile('<.*?>')
    html_pattern.sub(r'', text)

    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')
    text = tokenizer.tokenize(text)
    text = " ".join(text)
    return text
```


```python
df = pd.read_csv("./걸그룹 가사 영어 번역 추가.csv")
df['cleaned'] = df['LyricsEn'].apply(junchuri)
df.head(3)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>가수</th>
      <th>히트곡</th>
      <th>고유번호</th>
      <th>Lyrics</th>
      <th>LyricsEn</th>
      <th>cleaned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>브라운아이드걸스</td>
      <td>어쩌다</td>
      <td>1598921</td>
      <td>* 어쩌다 어쩌다 어쩌다 널 사랑하게 됐는지 내가 왜 이꼴이 됐는지 어쩌다 어쩌다 ...</td>
      <td>* I didn't know how I fell in love with you Ho...</td>
      <td>I know I fell love How I fell love How I got w...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>브라운아이드걸스</td>
      <td>Abracadabra</td>
      <td>1775962</td>
      <td>이러다 미쳐 내가 여리 여리 착하던 그런 내가 너 때문에 돌아 내가 독한 나로 변해...</td>
      <td>Every night I'll be with you, do you love her,...</td>
      <td>Every night I ll you love her love me love me ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>브라운아이드걸스</td>
      <td>Love</td>
      <td>1003682</td>
      <td>야릇야릇한 널 향한 나의 맘 들리니 I need you I love you 이런이런...</td>
      <td>Do you hear my strange heart for you I need yo...</td>
      <td>Do hear strange heart I need I love I need I n...</td>
    </tr>
  </tbody>
</table>
</div>



```python
# 전처리 과정에서 빈 값이 생긴 행이 있다면, nan 값으로 변환 후에 해당 행을 제거
df['cleaned'].replace('', np.nan, inplace=True)
df = df[df['cleaned'].notna()]
print('전체 문서의 수 :',len(df))
```

<pre>
전체 문서의 수 : 206
</pre>

```python
girl = pd.DataFrame(df['가수'].value_counts()).reset_index()
girl.columns = ['가수', '곡 수']
girl
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>가수</th>
      <th>곡 수</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>아이오아이</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2NE1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>블랙핑크</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kep1er</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>LE SSERAFIM</td>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>씨스타</td>
      <td>5</td>
    </tr>
    <tr>
      <th>6</th>
      <td>크레용팝</td>
      <td>5</td>
    </tr>
    <tr>
      <th>7</th>
      <td>f(x)</td>
      <td>5</td>
    </tr>
    <tr>
      <th>8</th>
      <td>라붐</td>
      <td>5</td>
    </tr>
    <tr>
      <th>9</th>
      <td>러블리즈</td>
      <td>5</td>
    </tr>
    <tr>
      <th>10</th>
      <td>써니힐</td>
      <td>5</td>
    </tr>
    <tr>
      <th>11</th>
      <td>모모랜드</td>
      <td>5</td>
    </tr>
    <tr>
      <th>12</th>
      <td>레이디스 코드</td>
      <td>5</td>
    </tr>
    <tr>
      <th>13</th>
      <td>애프터스쿨</td>
      <td>5</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Red Velvet</td>
      <td>5</td>
    </tr>
    <tr>
      <th>15</th>
      <td>4minute</td>
      <td>5</td>
    </tr>
    <tr>
      <th>16</th>
      <td>씨야</td>
      <td>5</td>
    </tr>
    <tr>
      <th>17</th>
      <td>AOA</td>
      <td>5</td>
    </tr>
    <tr>
      <th>18</th>
      <td>오마이걸</td>
      <td>5</td>
    </tr>
    <tr>
      <th>19</th>
      <td>TWICE</td>
      <td>5</td>
    </tr>
    <tr>
      <th>20</th>
      <td>STAYC</td>
      <td>5</td>
    </tr>
    <tr>
      <th>21</th>
      <td>걸스데이</td>
      <td>5</td>
    </tr>
    <tr>
      <th>22</th>
      <td>시크릿</td>
      <td>5</td>
    </tr>
    <tr>
      <th>23</th>
      <td>미스에이</td>
      <td>5</td>
    </tr>
    <tr>
      <th>24</th>
      <td>브라운아이드걸스</td>
      <td>5</td>
    </tr>
    <tr>
      <th>25</th>
      <td>(여자)아이들</td>
      <td>5</td>
    </tr>
    <tr>
      <th>26</th>
      <td>여자친구</td>
      <td>5</td>
    </tr>
    <tr>
      <th>27</th>
      <td>레인보우</td>
      <td>5</td>
    </tr>
    <tr>
      <th>28</th>
      <td>ITZY</td>
      <td>5</td>
    </tr>
    <tr>
      <th>29</th>
      <td>마마무</td>
      <td>5</td>
    </tr>
    <tr>
      <th>30</th>
      <td>소녀시대</td>
      <td>5</td>
    </tr>
    <tr>
      <th>31</th>
      <td>IZ*ONE</td>
      <td>5</td>
    </tr>
    <tr>
      <th>32</th>
      <td>원더걸스</td>
      <td>5</td>
    </tr>
    <tr>
      <th>33</th>
      <td>브레이브걸스</td>
      <td>5</td>
    </tr>
    <tr>
      <th>34</th>
      <td>EXID</td>
      <td>5</td>
    </tr>
    <tr>
      <th>35</th>
      <td>aespa</td>
      <td>5</td>
    </tr>
    <tr>
      <th>36</th>
      <td>카라</td>
      <td>5</td>
    </tr>
    <tr>
      <th>37</th>
      <td>에이핑크</td>
      <td>5</td>
    </tr>
    <tr>
      <th>38</th>
      <td>티아라</td>
      <td>5</td>
    </tr>
    <tr>
      <th>39</th>
      <td>달샤벳</td>
      <td>5</td>
    </tr>
    <tr>
      <th>40</th>
      <td>IVE</td>
      <td>4</td>
    </tr>
    <tr>
      <th>41</th>
      <td>NMIXX</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



```python
# 가사별 단어수 그래프 -> 500-1000자 사이가 가장 많았음. 
import matplotlib.pyplot as plt
# %matplotlip inline
plt.hist(df['cleaned'].str.len())
plt.show()
```

<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANpklEQVR4nO3dX4xc51nH8e8PJ21Rmqg2WVurJGITZBUipPxhFVIFRQKTkiYIm4ugVAJWyJJvWpRKILSlN+XORaKiCFTJJIEFQkvUNrLVqKXW0qhCitKuW+cfTnASTGpivNuUqikXLUkfLuaYLpvZ7Oyf2fG7+/1Iq3POO2d8nn00/un4nXnHqSokSe35sVEXIElaGwNckhplgEtSowxwSWqUAS5JjbpkMy925ZVX1sTExGZeUpKad+LEiW9V1djS8U0N8ImJCebm5jbzkpLUvCT/3m/cKRRJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWrUpq7EVBsmph8d2bXPHL57ZNeWWuMduCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRLuS5iI1yQY2ki5934JLUKANckhplgEtSowxwSWqUAS5JjRoowJO8K8lnkjyX5FSS9yTZleR4ktPdduewi5Uk/cigd+CfAL5YVT8N3ACcAqaB2araC8x2x5KkTbJigCe5ArgdeACgqn5QVd8B9gMz3WkzwIHhlChJ6meQO/DrgAXgr5J8I8n9SS4D9lTVOYBuu3uIdUqSlhgkwC8BbgY+WVU3Af/NKqZLkhxKMpdkbmFhYY1lSpKWGiTAzwJnq+qJ7vgz9AL9fJJxgG473+/JVXWkqiaranJsbGwjapYkMUCAV9V/At9M8u5uaB/wL8AxYKobmwKODqVCSVJfg36Z1e8CDyV5G/AS8Dv0wv/hJAeBl4F7hlOiJKmfgQK8qk4Ck30e2reh1UiSBuZKTElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIadckgJyU5A7wGvAG8XlWTSXYB/wBMAGeA36iq/xpOmZKkpVZzB/6LVXVjVU12x9PAbFXtBWa7Y0nSJlnPFMp+YKbbnwEOrLsaSdLABg3wAr6U5ESSQ93Ynqo6B9Btd/d7YpJDSeaSzC0sLKy/YkkSMOAcOHBbVb2SZDdwPMlzg16gqo4ARwAmJydrDTVKkvoY6A68ql7ptvPAI8AtwPkk4wDddn5YRUqS3mzFAE9yWZLLL+wD7wWeAY4BU91pU8DRYRUpSXqzQaZQ9gCPJLlw/t9X1ReTfA14OMlB4GXgnuGVKUlaasUAr6qXgBv6jL8K7BtGUZKklbkSU5IaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNWrgAE+yI8k3kny+O96V5HiS09125/DKlCQttZo78PuAU4uOp4HZqtoLzHbHkqRNMlCAJ7kauBu4f9HwfmCm258BDmxoZZKktzToHfifAn8A/HDR2J6qOgfQbXf3e2KSQ0nmkswtLCysp1ZJ0iIrBniSXwXmq+rEWi5QVUeqarKqJsfGxtbyR0iS+rhkgHNuA34tyV3AO4ArkvwdcD7JeFWdSzIOzA+zUEnS/7fiHXhVfbiqrq6qCeBe4J+q6jeBY8BUd9oUcHRoVUqS3mQ9nwM/DNyR5DRwR3csSdokg0yh/J+qegx4rNt/Fdi38SVJkgbhSkxJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjVrVUnpp2CamHx3Jdc8cvnsk15XWwztwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDVqxQBP8o4kX03yZJJnk/xRN74ryfEkp7vtzuGXK0m6YJA78O8Dv1RVNwA3AncmuRWYBmarai8w2x1LkjbJigFePd/rDi/tfgrYD8x04zPAgWEUKEnqb6A58CQ7kpwE5oHjVfUEsKeqzgF0293LPPdQkrkkcwsLCxtUtiRpoACvqjeq6kbgauCWJD876AWq6khVTVbV5NjY2BrLlCQttapPoVTVd4DHgDuB80nGAbrt/EYXJ0la3iCfQhlL8q5u/8eBXwaeA44BU91pU8DRIdUoSepjkP8TcxyYSbKDXuA/XFWfT/I48HCSg8DLwD1DrFOStMSKAV5VTwE39Rl/Fdg3jKIkSStzJaYkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWrUigGe5JokX05yKsmzSe7rxnclOZ7kdLfdOfxyJUkXDHIH/jrwe1X1M8CtwAeSXA9MA7NVtReY7Y4lSZtkxQCvqnNV9fVu/zXgFHAVsB+Y6U6bAQ4MqUZJUh+rmgNPMgHcBDwB7Kmqc9ALeWD3Ms85lGQuydzCwsI6y5UkXTBwgCd5J/BZ4ENV9d1Bn1dVR6pqsqomx8bG1lKjJKmPgQI8yaX0wvuhqvpcN3w+yXj3+DgwP5wSJUn9DPIplAAPAKeq6uOLHjoGTHX7U8DRjS9PkrScSwY45zbgt4Cnk5zsxv4QOAw8nOQg8DJwz1AqlCT1tWKAV9U/A1nm4X0bW44kaVCuxJSkRhngktQoA1ySGjXIm5jSljcx/ejIrn3m8N0ju7ba5h24JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhrll1kNYJRfdCRJy/EOXJIaZYBLUqMMcElqlAEuSY0ywCWpUSsGeJIHk8wneWbR2K4kx5Oc7rY7h1umJGmpQe7A/xq4c8nYNDBbVXuB2e5YkrSJVgzwqvoK8O0lw/uBmW5/BjiwsWVJklay1jnwPVV1DqDb7l7uxCSHkswlmVtYWFjj5SRJSw39TcyqOlJVk1U1OTY2NuzLSdK2sdYAP59kHKDbzm9cSZKkQaw1wI8BU93+FHB0Y8qRJA1qkI8Rfgp4HHh3krNJDgKHgTuSnAbu6I4lSZtoxW8jrKr3L/PQvg2uRZK0Cq7ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktSoFZfSSxquielHR3LdM4fvHsl1tXG8A5ekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEY1sxJzVKvVpK1qlH+nRrUKdKv9zt6BS1KjDHBJatS6plCS3Al8AtgB3F9VhzekKklbmlOiG2PNd+BJdgB/AbwPuB54f5LrN6owSdJbW88Uyi3AC1X1UlX9APg0sH9jypIkrWQ9UyhXAd9cdHwW+PmlJyU5BBzqDr+X5Pm3+DOvBL61jpq2KvuyPHvTn31Z3kh6k4+t6+k/2W9wPQGePmP1poGqI8CRgf7AZK6qJtdR05ZkX5Znb/qzL8vbSr1ZzxTKWeCaRcdXA6+srxxJ0qDWE+BfA/YmuTbJ24B7gWMbU5YkaSVrnkKpqteTfBD4R3ofI3ywqp5dZz0DTbVsQ/ZlefamP/uyvC3Tm1S9adpaktQAV2JKUqMMcElq1EUR4EnuTPJ8kheSTI+6nlFIcibJ00lOJpnrxnYlOZ7kdLfduej8D3f9ej7Jr4yu8o2V5MEk80meWTS26j4k+bmuny8k+bMk/T722pRlevPRJP/RvW5OJrlr0WPbojdJrkny5SSnkjyb5L5ufOu/bqpqpD/03gB9EbgOeBvwJHD9qOsaQR/OAFcuGftjYLrbnwY+1u1f3/Xp7cC1Xf92jPp32KA+3A7cDDyznj4AXwXeQ2+9wheA9436dxtSbz4K/H6fc7dNb4Bx4OZu/3LgX7vff8u/bi6GO3CX5C9vPzDT7c8ABxaNf7qqvl9V/wa8QK+PzauqrwDfXjK8qj4kGQeuqKrHq/e38m8WPadZy/RmOdumN1V1rqq+3u2/Bpyit1J8y79uLoYA77ck/6oR1TJKBXwpyYnu6wcA9lTVOei9SIHd3fh269lq+3BVt790fKv6YJKnuimWC9ME27I3SSaAm4An2Aavm4shwAdakr8N3FZVN9P7dscPJLn9Lc61Zz3L9WE79eeTwE8BNwLngD/pxrddb5K8E/gs8KGq+u5bndpnrMneXAwB7pJ8oKpe6bbzwCP0pkTOd/+so9vOd6dvt56ttg9nu/2l41tOVZ2vqjeq6ofAX/KjqbRt1Zskl9IL74eq6nPd8JZ/3VwMAb7tl+QnuSzJ5Rf2gfcCz9Drw1R32hRwtNs/Btyb5O1JrgX20nvzZataVR+6fy6/luTW7lMEv73oOVvKhYDq/Dq91w1so950v8cDwKmq+viih7b+62bU76J27/zeRe+d4xeBj4y6nhH8/tfRe1f8SeDZCz0AfgKYBU53212LnvORrl/Pc5G/U77KXnyK3lTA/9C7Izq4lj4Ak/TC7EXgz+lWHbf8s0xv/hZ4GniKXjCNb7feAL9Ab6rjKeBk93PXdnjduJRekhp1MUyhSJLWwACXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjfpfEDykVaNCy8kAAAAASUVORK5CYII="/>

---

### Word2Vec



```python
corpus = []

for words in df['cleaned']:
    corpus.append(words.split())
```


```python
# 분할된 부분도 df에 추가 
df['split'] = 0 
for i in range(len(df)):
    df['split'][i] = corpus[i] 
```

<pre>
<ipython-input-12-09e94ffc430f>:4: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df['split'][i] = corpus[i]
/Users/Wille/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self._setitem_single_block(indexer, value, name)
</pre>

```python
def get_document_vectors(document_list):
    document_embedding_list = []

    # 각 문서에 대해서
    for line in document_list:
        doc2vec = None
        count = 0
        for word in line.split():
#             if word in word2vec_model.wv.vocab:
            if word in word2vec_model.key_to_index:

                count += 1
                # 해당 문서에 있는 모든 단어들의 벡터값을 더함
                if doc2vec is None:
                    doc2vec = word2vec_model[word]
                else:
                    doc2vec = doc2vec + word2vec_model[word]

        if doc2vec is not None:
            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠줌
            doc2vec = doc2vec / count
            document_embedding_list.append(doc2vec)
    # 각 문서에 대한 문서 벡터 리스트를 리턴
    return document_embedding_list
```


```python
document_embedding_list = get_document_vectors(df['cleaned'])
print('문서 벡터의 수 :',len(document_embedding_list))
```

<pre>
문서 벡터의 수 : 206
</pre>

```python
# 각 문서 벡터 간 코사인 유사도를 계산
cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)
print('코사인 유사도 매트릭스의 크기 :',cosine_similarities.shape)
```

<pre>
코사인 유사도 매트릭스의 크기 : (206, 206)
</pre>
---

### 코사인 유사도 Cos similarity

벡터와 벡터 간의 유사도 비교

* 교수님 책 추천 코드 이해 https://wikidocs.net/24603



```python
def recom_cos(text) : 
    text = junchuri(text)
    
    # 데이터에 추가하여 검사하기 때문에 다른 변수로 불러옴
    dfdf = pd.read_csv("./걸그룹 가사 영어 번역 추가.csv")
    dfdf['cleaned'] = dfdf['LyricsEn'].apply(junchuri)

    # 전처리 과정에서 빈 값이 생긴 행이 있다면, nan 값으로 변환 후에 해당 행을 제거
    dfdf['cleaned'].replace('', np.nan, inplace=True)
    dfdf = dfdf[dfdf['cleaned'].notna()]
#     print('전체 문서의 수 :',len(dfdf))

    # dfdf = df.copy()
    i= len(dfdf)
    dfdf.at[i] = ['0','0',"0",'0','0',text]
#     print(len(dfdf))

    corpus = []

    for words in dfdf['cleaned']:
        corpus.append(words.split())

    # 분할된 부분도 df에 추가 
    dfdf['split'] = 0 
    for i in range(len(dfdf)):
        dfdf['split'][i] = corpus[i] 

    document_embedding_list = get_document_vectors(dfdf['cleaned'])
#     print('문서 벡터의 수 :',len(document_embedding_list))

    # 각 문서 벡터 간 코사인 유사도를 계산
    cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)
#     print('코사인 유사도 매트릭스의 크기 :',cosine_similarities.shape)
    
    songs = dfdf[['가수', '히트곡']]

    # 노래가사을 입력하면 해당 제목의 인덱스를 리턴받아 idx에 저장.
    indices = pd.Series(dfdf.index, index = dfdf['cleaned']).drop_duplicates()    
    idx = indices[text]

    # 입력된 노래가사가 유사한 5개 선정.
    sim_scores = list(enumerate(cosine_similarities[idx]))
    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)
    sim_scores = sim_scores[1:6]

    # 가장 유사한 노래 5개
    song_indices = [i[0] for i in sim_scores]

    # 전체 데이터프레임에서 해당 인덱스의 행만 추출. 5개의 행을 가진다.
    recommend = songs.iloc[song_indices].reset_index(drop=True)
    
    print("===[ 코사인 유사도 검사 ]===")
    for i in range(5):
        print(i+1, '번째 추천곡 : ', recommend['가수'][i], '의', recommend['히트곡'][i])  
```

---

###  자카드 유사도 Jaccard Similarity

벡터 값이 아닌 집합 개념



```python
# 자카드 유사도 구하는 함수
def jaccardd(text, a) :
    for i in range(len(df)) :
        set_a = set(text.split())

        union = set_a.union(a)
        intersection = set_a.intersection(a)
#     print('합집합 : ', union)
#     print('교집합 : ', intersection)
        jacc = len(intersection)/len(union)
    return jacc
```


```python
def recom_jacc(text) : 
    jac = pd.DataFrame(index=range(0, len(df)), columns={'가수', '히트곡', '자카드유사도'})
    for i in range(len(df)):
        a = df['split'][i]
        jac['가수'][i] = df['가수'][i]
        jac['히트곡'][i] = df['히트곡'][i]
        jac["자카드유사도"][i] = jaccardd(text, a)
    #     print(jac["자카드유사도"][i])
    ja = jac.sort_values(by='자카드유사도', ascending =False).reset_index()

    print("===[ 자카드 유사도 검사 ]===")
    for i in range(5):
        print(i+1, '번째 추천곡 : ', ja['가수'][i], '의', ja['히트곡'][i])
```


```python
```

---

### 파파고 한국어->영어 번역 파트



```python
# 네이버 api 파파고 번역 반자동화
import os
import sys
import requests
from pprint import pprint
import time

client_id = '0NVy5n6c0srx679d9kWr'
client_secret = 'Vf9HL4rJ37'

def get_translate(text):
    data = {'text' : text,
            'source' : 'ko',
            'target': 'en'}

    url = "https://openapi.naver.com/v1/papago/n2mt"

    header = {"X-Naver-Client-Id":client_id,
              "X-Naver-Client-Secret":client_secret}

    response = requests.post(url, headers=header, data= data)
    rescode = response.status_code

    if(rescode==200):
        t_data = response.json()    
        result = t_data['message']['result']['translatedText']
        en_text = result
    else:
        print("Error Code:" , rescode)
        
    return en_text
```

---

### 결론



```python
test_sample=pd.read_csv("./test_sample.csv")
test_sample
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>가수-제목</th>
      <th>가사</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>아이들 - 달라</td>
      <td>Dollar, dollar, dollar\nDollar, dollar, dollar...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>스테이시 - 사랑은 어찌고</td>
      <td>Hoo, hoo\nFell in love\nClose your eyes gently...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>아이들-사랑해</td>
      <td>Don't you giddily laughing.\nIt's a little bit...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>미스에이-다른남자말고너</td>
      <td>Not another man, but you\nNot another man (oh ...</td>
    </tr>
  </tbody>
</table>
</div>



```python
# 아이들- 달라 : 한국어 가사 test
recom_cos(get_translate(test_sample['가사'][0]))
recom_jacc( get_translate(test_sample['가사'][0]))
```

<pre>
<ipython-input-16-0a6dbbe7b0c7>:26: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  dfdf['split'][i] = corpus[i]
/Users/Wille/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self._setitem_single_block(indexer, value, name)
</pre>
<pre>
===[ 코사인 유사도 검사 ]===
1 번째 추천곡 :  STAYC 의 색안경 (STEREOTYPE)
2 번째 추천곡 :  2NE1 의 Go Away
3 번째 추천곡 :  원더걸스  의 Be My Baby
4 번째 추천곡 :  브레이브걸스 의 어쩌다 2
5 번째 추천곡 :  STAYC 의 SAME SAME
===[ 자카드 유사도 검사 ]===
1 번째 추천곡 :  크레용팝 의 Bing Bing
2 번째 추천곡 :  달샤벳 의 있기 없기
3 번째 추천곡 :  브라운아이드걸스 의 어쩌다
4 번째 추천곡 :  아이오아이 의 너무너무너무
5 번째 추천곡 :  ITZY 의  WANNABE
</pre>

```python
# text2 - 스테이시 - 사랑은...힘든건가요

recom_cos(test_sample['가사'][1])
recom_jacc(test_sample['가사'][1])
```

<pre>
<ipython-input-16-0a6dbbe7b0c7>:26: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  dfdf['split'][i] = corpus[i]
/Users/Wille/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self._setitem_single_block(indexer, value, name)
</pre>
<pre>
===[ 코사인 유사도 검사 ]===
1 번째 추천곡 :  블랙핑크 의 Lovesick Girls
2 번째 추천곡 :  여자친구 의 밤 (Time for the moon night)
3 번째 추천곡 :  AOA 의 심쿵해 (Heart Attack)
4 번째 추천곡 :  씨야 의 결혼할까요
5 번째 추천곡 :  달샤벳 의 있기 없기
===[ 자카드 유사도 검사 ]===
1 번째 추천곡 :  티아라 의 SEXY LOVE
2 번째 추천곡 :  IVE 의  LOVE DIVE
3 번째 추천곡 :  달샤벳 의 있기 없기
4 번째 추천곡 :  여자친구 의 유리구슬 (Glass Bead)
5 번째 추천곡 :  크레용팝 의 Bing Bing
</pre>

```python
# text2 - 아이들, 사랑해

recom_cos(test_sample['가사'][2])
recom_jacc(test_sample['가사'][2])
```

<pre>
<ipython-input-16-0a6dbbe7b0c7>:26: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  dfdf['split'][i] = corpus[i]
/Users/Wille/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self._setitem_single_block(indexer, value, name)
</pre>
<pre>
===[ 코사인 유사도 검사 ]===
1 번째 추천곡 :  씨야 의 결혼할까요
2 번째 추천곡 :  블랙핑크 의 Lovesick Girls
3 번째 추천곡 :  2NE1 의 Go Away
4 번째 추천곡 :  원더걸스  의 Be My Baby
5 번째 추천곡 :  레인보우 의 Tell Me Tell Me
===[ 자카드 유사도 검사 ]===
1 번째 추천곡 :  크레용팝 의 Bing Bing
2 번째 추천곡 :  AOA 의 심쿵해 (Heart Attack)
3 번째 추천곡 :  라붐 의 아로아로
4 번째 추천곡 :  STAYC 의 색안경 (STEREOTYPE)
5 번째 추천곡 :  IVE 의  LOVE DIVE
</pre>

```python
# text3 - 미스에이, 다른남자말고너(학습된 값)

recom_cos(test_sample['가사'][3])
recom_jacc(test_sample['가사'][3])
```

<pre>
<ipython-input-16-0a6dbbe7b0c7>:26: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  dfdf['split'][i] = corpus[i]
/Users/Wille/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  self._setitem_single_block(indexer, value, name)
</pre>
<pre>
===[ 코사인 유사도 검사 ]===
1 번째 추천곡 :  미스에이 의 다른 남자 말고 너
2 번째 추천곡 :  STAYC 의 SAME SAME
3 번째 추천곡 :  달샤벳 의 있기 없기
4 번째 추천곡 :  원더걸스  의 Be My Baby
5 번째 추천곡 :  2NE1 의 Go Away
===[ 자카드 유사도 검사 ]===
1 번째 추천곡 :  미스에이 의 다른 남자 말고 너
2 번째 추천곡 :  STAYC 의 SO BAD
3 번째 추천곡 :  크레용팝 의 Bing Bing
4 번째 추천곡 :  AOA 의 심쿵해 (Heart Attack)
5 번째 추천곡 :  STAYC 의 SAME SAME
</pre>

```python
```


```python
```


```python
```


```python
```
